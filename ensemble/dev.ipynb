{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "557df603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "__author__ = 'Author'\n",
    "__email__ = 'Email'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af65fa",
   "metadata": {},
   "source": [
    "# SemEval 2026 Task 5 - Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84dcf5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# dependency\n",
    "# built-in\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# third-party\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from xgboost import XGBRanker, XGBRegressor\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# local - add src/eval to path for importing evaluation functions\n",
    "sys.path.insert(0, str(Path('../src/eval').resolve()))\n",
    "# Import evaluation functions from src/eval/scoring.py\n",
    "import scoring\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b742876",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c7a896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_predictions(filepath):\n",
    "    \"\"\"Load predictions from a JSONL file into a dictionary.\"\"\"\n",
    "    predictions = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            predictions[data['id']] = data['prediction']\n",
    "    return predictions\n",
    "\n",
    "# Evaluation functions following the structure of src/eval/scoring.py\n",
    "def evaluate_predictions_array(y_pred, y_true_labels):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using the same logic as scoring.py\n",
    "    \n",
    "    Args:\n",
    "        y_pred: array of predictions\n",
    "        y_true_labels: list of gold label lists (5 ratings each)\n",
    "    \n",
    "    Returns:\n",
    "        dict with spearman and accuracy scores\n",
    "    \"\"\"\n",
    "    # Build prediction and gold lists (same structure as scoring.py)\n",
    "    pred_list = list(y_pred)\n",
    "    gold_list = [scoring.get_average(labels) for labels in y_true_labels]\n",
    "    \n",
    "    # Calculate Spearman correlation (same as scoring.py)\n",
    "    corr, p_value = spearmanr(pred_list, gold_list)\n",
    "    \n",
    "    # Calculate accuracy within SD (same logic as scoring.py)\n",
    "    correct_guesses = 0\n",
    "    wrong_guesses = 0\n",
    "    \n",
    "    for pred, labels in zip(pred_list, y_true_labels):\n",
    "        if scoring.is_within_standard_deviation(pred, labels):\n",
    "            correct_guesses += 1\n",
    "        else:\n",
    "            wrong_guesses += 1\n",
    "    \n",
    "    accuracy = correct_guesses / (correct_guesses + wrong_guesses)\n",
    "    \n",
    "    return {\n",
    "        'spearman': corr,\n",
    "        'p_value': p_value,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct_guesses,\n",
    "        'total': correct_guesses + wrong_guesses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d6f7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06797395",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22c6607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 588 gold labels\n",
      "\n",
      "Gold labels are lists of 5 human ratings (1-5 scale)\n",
      "\n",
      "Example gold labels:\n",
      "  ID 0: [4, 5, 3, 1, 5] (avg=3.60, std=1.67)\n",
      "  ID 1: [3, 3, 4, 4, 4] (avg=3.60, std=0.55)\n",
      "  ID 2: [5, 5, 2, 3, 4] (avg=3.80, std=1.30)\n",
      "  ID 3: [4, 5, 4, 3, 5] (avg=4.20, std=0.84)\n",
      "  ID 4: [1, 5, 4, 4, 1] (avg=3.00, std=1.87)\n"
     ]
    }
   ],
   "source": [
    "# Load gold labels (solution file)\n",
    "SOLUTION_FILE = Path(\"../res/data/dev_solution.jsonl\")\n",
    "\n",
    "gold_labels = {}\n",
    "with open(SOLUTION_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        gold_labels[data['id']] = data['label']\n",
    "\n",
    "print(f\"Loaded {len(gold_labels)} gold labels\")\n",
    "print(f\"\\nGold labels are lists of 5 human ratings (1-5 scale)\")\n",
    "print(f\"\\nExample gold labels:\")\n",
    "for i in range(5):\n",
    "    sample_id = str(i)\n",
    "    print(f\"  ID {sample_id}: {gold_labels[sample_id]} (avg={np.mean(gold_labels[sample_id]):.2f}, std={np.std(gold_labels[sample_id], ddof=1):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a52dc",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb32e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 prediction files:\n",
      "  - chatgpt.jsonl\n",
      "  - david_v1.jsonl\n",
      "  - david_v2.jsonl\n",
      "  - korean.jsonl\n",
      "  - urdu_v1.jsonl\n",
      "  - urdu_v2.jsonl\n",
      "  - urdu_v3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Path to individual system outputs\n",
    "RESULTS_DIR = Path(\"../res/results/dev/\")\n",
    "\n",
    "# Get all jsonl files\n",
    "prediction_files = sorted(RESULTS_DIR.glob(\"*.jsonl\"))\n",
    "print(f\"Found {len(prediction_files)} prediction files:\")\n",
    "for f in prediction_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "59e1d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 588 predictions from chatgpt\n",
      "Loaded 588 predictions from david_v1\n",
      "Loaded 588 predictions from david_v2\n",
      "Loaded 588 predictions from korean\n",
      "Loaded 588 predictions from urdu_v1\n",
      "Loaded 588 predictions from urdu_v2\n",
      "Loaded 588 predictions from urdu_v3\n",
      "\n",
      "Predictions DataFrame shape: (588, 7)\n",
      "Number of samples: 588\n",
      "Number of systems: 7\n",
      "\n",
      "Systems: ['chatgpt', 'david_v1', 'david_v2', 'korean', 'urdu_v1', 'urdu_v2', 'urdu_v3']\n",
      "\n",
      "Sample data types:\n",
      "chatgpt       int64\n",
      "david_v1      int64\n",
      "david_v2    float64\n",
      "korean      float64\n",
      "urdu_v1       int64\n",
      "urdu_v2       int64\n",
      "urdu_v3       int64\n",
      "dtype: object\n",
      "\n",
      "First 10 predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatgpt</th>\n",
       "      <th>david_v1</th>\n",
       "      <th>david_v2</th>\n",
       "      <th>korean</th>\n",
       "      <th>urdu_v1</th>\n",
       "      <th>urdu_v2</th>\n",
       "      <th>urdu_v3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0600</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.9400</td>\n",
       "      <td>2.586742</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9976</td>\n",
       "      <td>2.647276</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0024</td>\n",
       "      <td>3.392001</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5480</td>\n",
       "      <td>2.816402</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.4520</td>\n",
       "      <td>3.373284</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5992</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4008</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.8696</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.1304</td>\n",
       "      <td>3.450690</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chatgpt  david_v1  david_v2    korean  urdu_v1  urdu_v2  urdu_v3\n",
       "id                                                                  \n",
       "0         5         4    4.0600  4.360252        4        4        4\n",
       "1         2         4    1.9400  2.586742        3        2        3\n",
       "2         3         5    3.9976  2.647276        4        3        4\n",
       "3         2         5    2.0024  3.392001        4        4        4\n",
       "4         3         5    3.5480  2.816402        4        4        4\n",
       "5         2         4    2.4520  3.373284        4        3        4\n",
       "6         5         4    4.5992  4.360252        2        4        2\n",
       "7         1         1    1.4008  1.942182        2        3        2\n",
       "8         1         3    2.8696  1.942182        2        2        2\n",
       "9         3         3    3.1304  3.450690        4        5        4"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all predictions\n",
    "all_predictions = {}\n",
    "for pred_file in prediction_files:\n",
    "    system_name = pred_file.stem  # filename without extension\n",
    "    all_predictions[system_name] = load_predictions(pred_file)\n",
    "    print(f\"Loaded {len(all_predictions[system_name])} predictions from {system_name}\")\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "# Each row is a sample, each column is a system's prediction\n",
    "df_predictions = pd.DataFrame(all_predictions)\n",
    "df_predictions.index.name = 'id'\n",
    "\n",
    "print(f\"\\nPredictions DataFrame shape: {df_predictions.shape}\")\n",
    "print(f\"Number of samples: {len(df_predictions)}\")\n",
    "print(f\"Number of systems: {len(df_predictions.columns)}\")\n",
    "print(f\"\\nSystems: {list(df_predictions.columns)}\")\n",
    "print(f\"\\nSample data types:\")\n",
    "print(df_predictions.dtypes)\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b49qoanf5i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (588, 10)\n",
      "\n",
      "Columns: ['chatgpt', 'david_v1', 'david_v2', 'korean', 'urdu_v1', 'urdu_v2', 'urdu_v3', 'gold_labels', 'gold_avg', 'gold_std']\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatgpt</th>\n",
       "      <th>david_v1</th>\n",
       "      <th>david_v2</th>\n",
       "      <th>korean</th>\n",
       "      <th>urdu_v1</th>\n",
       "      <th>urdu_v2</th>\n",
       "      <th>urdu_v3</th>\n",
       "      <th>gold_labels</th>\n",
       "      <th>gold_avg</th>\n",
       "      <th>gold_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0600</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 5, 3, 1, 5]</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.673320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.9400</td>\n",
       "      <td>2.586742</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 4, 4, 4]</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.547723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9976</td>\n",
       "      <td>2.647276</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 5, 2, 3, 4]</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.303840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0024</td>\n",
       "      <td>3.392001</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 5, 4, 3, 5]</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.836660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5480</td>\n",
       "      <td>2.816402</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 5, 4, 4, 1]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.870829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.4520</td>\n",
       "      <td>3.373284</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 3, 4, 1, 3]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.224745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5992</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 4, 5, 5, 5]</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.547723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4008</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1]</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.516398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.8696</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 1, 1, 2, 3]</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.303840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.1304</td>\n",
       "      <td>3.450690</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 2, 5, 4, 4]</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.095445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chatgpt  david_v1  david_v2    korean  urdu_v1  urdu_v2  urdu_v3  \\\n",
       "id                                                                     \n",
       "0         5         4    4.0600  4.360252        4        4        4   \n",
       "1         2         4    1.9400  2.586742        3        2        3   \n",
       "2         3         5    3.9976  2.647276        4        3        4   \n",
       "3         2         5    2.0024  3.392001        4        4        4   \n",
       "4         3         5    3.5480  2.816402        4        4        4   \n",
       "5         2         4    2.4520  3.373284        4        3        4   \n",
       "6         5         4    4.5992  4.360252        2        4        2   \n",
       "7         1         1    1.4008  1.942182        2        3        2   \n",
       "8         1         3    2.8696  1.942182        2        2        2   \n",
       "9         3         3    3.1304  3.450690        4        5        4   \n",
       "\n",
       "           gold_labels  gold_avg  gold_std  \n",
       "id                                          \n",
       "0      [4, 5, 3, 1, 5]  3.600000  1.673320  \n",
       "1      [3, 3, 4, 4, 4]  3.600000  0.547723  \n",
       "2      [5, 5, 2, 3, 4]  3.800000  1.303840  \n",
       "3      [4, 5, 4, 3, 5]  4.200000  0.836660  \n",
       "4      [1, 5, 4, 4, 1]  3.000000  1.870829  \n",
       "5      [4, 3, 4, 1, 3]  3.000000  1.224745  \n",
       "6      [4, 4, 5, 5, 5]  4.600000  0.547723  \n",
       "7   [1, 1, 1, 2, 2, 1]  1.333333  0.516398  \n",
       "8      [4, 1, 1, 2, 3]  2.200000  1.303840  \n",
       "9      [4, 2, 5, 4, 4]  3.800000  1.095445  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a combined DataFrame with predictions and gold labels\n",
    "df_gold = pd.DataFrame({\n",
    "    'gold_labels': gold_labels,\n",
    "    'gold_avg': {k: np.mean(v) for k, v in gold_labels.items()},\n",
    "    'gold_std': {k: np.std(v, ddof=1) for k, v in gold_labels.items()}\n",
    "})\n",
    "\n",
    "# Combine predictions with gold labels\n",
    "df_combined = df_predictions.join(df_gold)\n",
    "\n",
    "print(f\"Combined DataFrame shape: {df_combined.shape}\")\n",
    "print(f\"\\nColumns: {list(df_combined.columns)}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_combined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fj1yfb0wqcp",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a72891f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 588\n",
      "Training samples: 411 (69.9%)\n",
      "Test samples: 177 (30.1%)\n",
      "\n",
      "Training set gold_avg distribution:\n",
      "  Mean: 3.109\n",
      "  Std: 1.191\n",
      "  Min: 1.000\n",
      "  Max: 5.000\n",
      "\n",
      "Test set gold_avg distribution:\n",
      "  Mean: 3.140\n",
      "  Std: 1.178\n",
      "  Min: 1.000\n",
      "  Max: 5.000\n",
      "\n",
      "Training set sample IDs range: 0 to 97\n",
      "Test set sample IDs range: 1 to 99\n"
     ]
    }
   ],
   "source": [
    "# Use stratification based on binned gold_avg to ensure balanced distribution\n",
    "df_combined['gold_bin'] = pd.cut(df_combined['gold_avg'], bins=5, labels=False)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df_combined, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=df_combined['gold_bin']\n",
    ")\n",
    "\n",
    "# Drop the temporary binning column\n",
    "train_df = train_df.drop('gold_bin', axis=1)\n",
    "test_df = test_df.drop('gold_bin', axis=1)\n",
    "\n",
    "print(f\"Total samples: {len(df_combined)}\")\n",
    "print(f\"Training samples: {len(train_df)} ({len(train_df)/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(test_df)} ({len(test_df)/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set gold_avg distribution:\")\n",
    "print(f\"  Mean: {train_df['gold_avg'].mean():.3f}\")\n",
    "print(f\"  Std: {train_df['gold_avg'].std():.3f}\")\n",
    "print(f\"  Min: {train_df['gold_avg'].min():.3f}\")\n",
    "print(f\"  Max: {train_df['gold_avg'].max():.3f}\")\n",
    "\n",
    "print(f\"\\nTest set gold_avg distribution:\")\n",
    "print(f\"  Mean: {test_df['gold_avg'].mean():.3f}\")\n",
    "print(f\"  Std: {test_df['gold_avg'].std():.3f}\")\n",
    "print(f\"  Min: {test_df['gold_avg'].min():.3f}\")\n",
    "print(f\"  Max: {test_df['gold_avg'].max():.3f}\")\n",
    "\n",
    "print(f\"\\nTraining set sample IDs range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "print(f\"Test set sample IDs range: {test_df.index.min()} to {test_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dk6qijpzjlj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (411, 5)\n",
      "y_train shape: (411,)\n",
      "X_test shape: (177, 5)\n",
      "y_test shape: (177,)\n",
      "\n",
      "First few training samples:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>david_v1</th>\n",
       "      <th>david_v2</th>\n",
       "      <th>korean</th>\n",
       "      <th>urdu_v1</th>\n",
       "      <th>urdu_v2</th>\n",
       "      <th>gold_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0080</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>5</td>\n",
       "      <td>4.9928</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>2.2500</td>\n",
       "      <td>2.630037</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>5</td>\n",
       "      <td>4.2396</td>\n",
       "      <td>4.397248</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0592</td>\n",
       "      <td>2.008239</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     david_v1  david_v2    korean  urdu_v1  urdu_v2  gold_avg\n",
       "id                                                           \n",
       "267         2    1.0080  1.942182        4        2       1.2\n",
       "294         5    4.9928  4.360252        4        5       4.4\n",
       "65          4    2.2500  2.630037        2        4       1.8\n",
       "191         5    4.2396  4.397248        4        5       4.0\n",
       "391         2    1.0592  2.008239        2        3       1.4"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare training and test sets\n",
    "# X = system predictions (features), y = gold average (target)\n",
    "\n",
    "X_train = train_df[['david_v1', 'david_v2', 'korean', 'urdu_v1', 'urdu_v2']]\n",
    "# X_train = train_df[['david_v1', 'david_v2', 'korean']]\n",
    "y_train = train_df['gold_avg']\n",
    "\n",
    "X_test = test_df[['david_v1', 'david_v2', 'korean', 'urdu_v1', 'urdu_v2']]\n",
    "# X_test = test_df[['david_v1', 'david_v2', 'korean']]\n",
    "y_test = test_df['gold_avg']\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nFirst few training samples:\")\n",
    "pd.concat([X_train.head(), y_train.head()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab584e1",
   "metadata": {},
   "source": [
    "# System Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4675dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Individual System Performance on Test Set (30%)\n",
      "================================================================================\n",
      "System              Spearman     Accuracy   Correct/Total\n",
      "--------------------------------------------------------------------------------\n",
      "david_v1            30.8245%     61.0169%     108/177    \n",
      "david_v2            60.5211%     68.3616%     121/177    \n",
      "korean              82.7033%     90.3955%     160/177    \n",
      "urdu_v1             43.0128%     69.4915%     123/177    \n",
      "urdu_v2             60.1281%     70.6215%     125/177    \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each system on test set\n",
    "print(\"=\" * 80)\n",
    "print(\"Individual System Performance on Test Set (30%)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'System':<15} {'Spearman':>12} {'Accuracy':>12} {'Correct/Total':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "system_results = {}\n",
    "test_gold_labels = test_df['gold_labels'].tolist()\n",
    "\n",
    "for system in ['david_v1', 'david_v2', 'korean', 'urdu_v1', 'urdu_v2']:\n",
    "# for system in ['david_v1', 'david_v2', 'korean']:\n",
    "    # Get predictions from X_test\n",
    "    y_pred = X_test[system].values\n",
    "    \n",
    "    # Evaluate using official logic\n",
    "    scores = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "    system_results[system] = scores\n",
    "    \n",
    "    print(f\"{system:<15} {scores['spearman']:>12.4%} {scores['accuracy']:>12.4%} \"\n",
    "          f\"{scores['correct']:>7}/{scores['total']:<7}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qi5dte9168s",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76405638",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9699fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (float) - Spearman: 77.0697%, Accuracy: 73.4463%\n",
      "Mean (int)   - Spearman: 66.4454%, Accuracy: 67.7966%\n"
     ]
    }
   ],
   "source": [
    "# Simple Mean Ensemble: y_pred = (1/N) * sum(y_i)\n",
    "y_pred_mean = X_test.mean(axis=1).values\n",
    "\n",
    "# Evaluate float version\n",
    "scores_mean_float = evaluate_predictions_array(y_pred_mean, test_gold_labels)\n",
    "print(f\"Mean (float) - Spearman: {scores_mean_float['spearman']:.4%}, Accuracy: {scores_mean_float['accuracy']:.4%}\")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_mean_int = y_pred_mean.round().clip(1, 5).astype(int)\n",
    "scores_mean_int = evaluate_predictions_array(y_pred_mean_int, test_gold_labels)\n",
    "print(f\"Mean (int)   - Spearman: {scores_mean_int['spearman']:.4%}, Accuracy: {scores_mean_int['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21923b15",
   "metadata": {},
   "source": [
    "## Weighted Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e142616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Mean (float) - Spearman: 80.5767%, Accuracy: 71.7514%\n",
      "Weighted Mean (int)   - Spearman: 71.1430%, Accuracy: 67.2316%\n"
     ]
    }
   ],
   "source": [
    "# Weighted Mean: weights learned from train set (70%) based on Spearman performance\n",
    "train_gold_labels = train_df['gold_labels'].tolist()\n",
    "\n",
    "# Calculate weights based on train set performance (Spearman correlation)\n",
    "weights = []\n",
    "for system in ['david_v1', 'david_v2', 'korean', 'urdu_v1', 'urdu_v2']:\n",
    "# for system in ['david_v1', 'david_v2', 'korean']:\n",
    "    y_pred_train = X_train[system].values\n",
    "    scores_train = evaluate_predictions_array(y_pred_train, train_gold_labels)\n",
    "    weights.append(scores_train['spearman'])\n",
    "\n",
    "# Convert to numpy array and normalize to sum to 1\n",
    "weights = np.array(weights)\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "# Apply weighted average on test set\n",
    "y_pred_weighted = (X_test.values * weights).sum(axis=1)\n",
    "\n",
    "# Evaluate float version\n",
    "scores_weighted_float = evaluate_predictions_array(y_pred_weighted, test_gold_labels)\n",
    "print(f\"Weighted Mean (float) - Spearman: {scores_weighted_float['spearman']:.4%}, Accuracy: {scores_weighted_float['accuracy']:.4%}\")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_weighted_int = y_pred_weighted.round().clip(1, 5).astype(int)\n",
    "scores_weighted_int = evaluate_predictions_array(y_pred_weighted_int, test_gold_labels)\n",
    "print(f\"Weighted Mean (int)   - Spearman: {scores_weighted_int['spearman']:.4%}, Accuracy: {scores_weighted_int['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b802c",
   "metadata": {},
   "source": [
    "## Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b12elmi6l38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Voting - Spearman: 59.0033%, Accuracy: 74.5763%\n"
     ]
    }
   ],
   "source": [
    "# Majority Voting: most common rounded prediction across systems\n",
    "# Round predictions to integers (1-5 range)\n",
    "X_test_rounded = X_test.round().clip(1, 5).astype(int)\n",
    "\n",
    "# Get mode (most frequent value) for each row\n",
    "y_pred_majority = stats.mode(X_test_rounded, axis=1, keepdims=False)[0]\n",
    "\n",
    "# Evaluate\n",
    "scores_majority = evaluate_predictions_array(y_pred_majority, test_gold_labels)\n",
    "print(f\"Majority Voting - Spearman: {scores_majority['spearman']:.4%}, Accuracy: {scores_majority['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cp9my6eewls",
   "metadata": {},
   "source": [
    "## Weighted Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f81163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Majority Voting - Spearman: 69.6016%, Accuracy: 78.5311%\n"
     ]
    }
   ],
   "source": [
    "# Weighted Majority Voting: weighted voting using performance-based weights\n",
    "# Round test predictions to integers\n",
    "X_test_rounded = X_test.round().clip(1, 5).astype(int)\n",
    "\n",
    "# Use weights learned earlier (from Weighted Mean)\n",
    "y_pred_weighted_vote = []\n",
    "for idx in range(len(X_test_rounded)):\n",
    "    # Get predictions for this sample\n",
    "    votes = X_test_rounded.iloc[idx].values\n",
    "    \n",
    "    # Count weighted votes for each class (1-5)\n",
    "    vote_counts = {}\n",
    "    for vote, weight in zip(votes, weights):\n",
    "        vote_counts[vote] = vote_counts.get(vote, 0) + weight\n",
    "    \n",
    "    # Select class with highest weighted vote\n",
    "    winner = max(vote_counts.items(), key=lambda x: x[1])[0]\n",
    "    y_pred_weighted_vote.append(winner)\n",
    "\n",
    "y_pred_weighted_vote = np.array(y_pred_weighted_vote)\n",
    "\n",
    "# Evaluate\n",
    "scores_weighted_vote = evaluate_predictions_array(y_pred_weighted_vote, test_gold_labels)\n",
    "print(f\"Weighted Majority Voting - Spearman: {scores_weighted_vote['spearman']:.4%}, Accuracy: {scores_weighted_vote['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4abc10",
   "metadata": {},
   "source": [
    "## Linear Stacking (Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c008e8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Regression Stacking (alpha=0.1)\n",
      "Ridge (float) - Spearman: 84.5721%, Accuracy: 91.5254%\n",
      "Ridge (int)   - Spearman: 81.5111%, Accuracy: 84.7458%\n",
      "================================================================================\n",
      "Ridge Regression Stacking (alpha=1.0)\n",
      "Ridge (float) - Spearman: 84.5721%, Accuracy: 91.5254%\n",
      "Ridge (int)   - Spearman: 81.5111%, Accuracy: 84.7458%\n",
      "================================================================================\n",
      "Ridge Regression Stacking (alpha=10.0)\n",
      "Ridge (float) - Spearman: 84.5721%, Accuracy: 91.5254%\n",
      "Ridge (int)   - Spearman: 81.5111%, Accuracy: 84.7458%\n"
     ]
    }
   ],
   "source": [
    "# Linear Stacking: learn weights via Ridge Regression on train set\n",
    "for alpha in [0.1, 1.0, 10.0]:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Ridge Regression Stacking (alpha={alpha})\")\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "    # Evaluate float version\n",
    "    scores_ridge_float = evaluate_predictions_array(y_pred_ridge, test_gold_labels)\n",
    "    print(f\"Ridge (float) - Spearman: {scores_ridge_float['spearman']:.4%}, Accuracy: {scores_ridge_float['accuracy']:.4%}\")\n",
    "\n",
    "    # Evaluate integer version\n",
    "    y_pred_ridge_int = y_pred_ridge.round().clip(1, 5).astype(int)\n",
    "    scores_ridge_int = evaluate_predictions_array(y_pred_ridge_int, test_gold_labels)\n",
    "    print(f\"Ridge (int)   - Spearman: {scores_ridge_int['spearman']:.4%}, Accuracy: {scores_ridge_int['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae292105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david_v1             +0.1391\n",
      "david_v2             +0.1424\n",
      "korean               +0.8260\n",
      "urdu_v1              +0.0404\n",
      "urdu_v2              +0.0683\n"
     ]
    }
   ],
   "source": [
    "# Inspect Ridge Weights\n",
    "coef = ridge.coef_\n",
    "for name, w in zip(X_train.columns, coef):\n",
    "    print(f\"{name:20s} {w:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156pqmx8rme",
   "metadata": {},
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f9b1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_xgb(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    test_gold_labels,\n",
    "    *,\n",
    "    n_estimators=500,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,     # L1\n",
    "    reg_lambda=10.0,   # L2\n",
    "    random_state=42,\n",
    "):\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Fit with early stopping (VERY IMPORTANT)\n",
    "    xgb.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    y_pred = xgb.predict(X_test)\n",
    "\n",
    "    # Float eval\n",
    "    scores_float = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "\n",
    "    # Int eval\n",
    "    y_pred_int = y_pred.round().clip(1, 5).astype(int)\n",
    "    scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n",
    "\n",
    "    return scores_float, scores_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a64857a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth=1 lr=0.1 n=100 | Spearman=83.3860% Acc=93.2203%\n",
      "depth=1 lr=0.05 n=200 | Spearman=83.1523% Acc=92.6554%\n",
      "depth=1 lr=0.02 n=400 | Spearman=83.1028% Acc=93.7853%\n",
      "depth=2 lr=0.1 n=100 | Spearman=82.9433% Acc=91.5254%\n",
      "depth=2 lr=0.05 n=200 | Spearman=82.8727% Acc=91.5254%\n",
      "depth=2 lr=0.02 n=400 | Spearman=83.0162% Acc=92.6554%\n",
      "depth=3 lr=0.1 n=100 | Spearman=82.0758% Acc=92.0904%\n",
      "depth=3 lr=0.05 n=200 | Spearman=82.2284% Acc=91.5254%\n",
      "depth=3 lr=0.02 n=400 | Spearman=82.5424% Acc=91.5254%\n"
     ]
    }
   ],
   "source": [
    "depths = [1, 2, 3]\n",
    "\n",
    "configs = [\n",
    "    dict(learning_rate=0.1,  n_estimators=100),\n",
    "    dict(learning_rate=0.05, n_estimators=200),\n",
    "    dict(learning_rate=0.02, n_estimators=400),\n",
    "]\n",
    "\n",
    "regularizers = [\n",
    "    dict(reg_alpha=0.0, reg_lambda=1.0),\n",
    "    dict(reg_alpha=1.0, reg_lambda=10.0),\n",
    "    dict(reg_alpha=10.0, reg_lambda=50.0),\n",
    "]\n",
    "\n",
    "for depth in [1, 2, 3]:\n",
    "    for cfg in configs:\n",
    "        scores_float, scores_int = train_eval_xgb(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            test_gold_labels,\n",
    "            max_depth=depth,\n",
    "            **cfg\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"depth={depth} lr={cfg['learning_rate']} n={cfg['n_estimators']} | \"\n",
    "            f\"Spearman={scores_float['spearman']:.4%} \"\n",
    "            f\"Acc={scores_float['accuracy']:.4%}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9350302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost depth=1 (float) - Spearman: 83.1761%, Accuracy: 94.3503%\n",
      "XGBoost depth=1 (int)   - Spearman: 80.3693%, Accuracy: 85.8757%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost with competition-validated settings\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=100,          # matched to lr=0.1\n",
    "    max_depth=1,               # CRITICAL: best Spearman\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=50,       # strong regularization\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,             # L1 regularization\n",
    "    reg_lambda=10.0,           # L2 regularization\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train on training set\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate float version\n",
    "scores_xgb_float = evaluate_predictions_array(y_pred_xgb, test_gold_labels)\n",
    "print(\n",
    "    f\"XGBoost depth=1 (float) - \"\n",
    "    f\"Spearman: {scores_xgb_float['spearman']:.4%}, \"\n",
    "    f\"Accuracy: {scores_xgb_float['accuracy']:.4%}\"\n",
    ")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_xgb_int = y_pred_xgb.round().clip(1, 5).astype(int)\n",
    "scores_xgb_int = evaluate_predictions_array(y_pred_xgb_int, test_gold_labels)\n",
    "print(\n",
    "    f\"XGBoost depth=1 (int)   - \"\n",
    "    f\"Spearman: {scores_xgb_int['spearman']:.4%}, \"\n",
    "    f\"Accuracy: {scores_xgb_int['accuracy']:.4%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "zvuvrblfn3k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Feature Importances:\n",
      "david_v1             0.0657\n",
      "david_v2             0.2308\n",
      "korean               0.6123\n",
      "urdu_v1              0.0251\n",
      "urdu_v2              0.0661\n",
      "\n",
      "Normalized Contributions (%):\n",
      "david_v1               6.57%\n",
      "david_v2              23.08%\n",
      "korean                61.23%\n",
      "urdu_v1                2.51%\n",
      "urdu_v2                6.61%\n"
     ]
    }
   ],
   "source": [
    "# Inspect XGBoost Feature Importances\n",
    "importance = xgb.feature_importances_\n",
    "print(\"XGBoost Feature Importances:\")\n",
    "for name, imp in zip(X_train.columns, importance):\n",
    "    print(f\"{name:20s} {imp:.4f}\")\n",
    "    \n",
    "# Visualize as percentages\n",
    "print(\"\\nNormalized Contributions (%):\")\n",
    "total_importance = importance.sum()\n",
    "for name, imp in zip(X_train.columns, importance):\n",
    "    print(f\"{name:20s} {imp/total_importance:>7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k0dkxcpsqn",
   "metadata": {},
   "source": [
    "## XGBoost Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "wdtsm4hle1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to scale predictions back to 1-5 range\n",
    "def scale_to_range(arr, low=1, upp=5):\n",
    "    \"\"\"Scale array values to be within the specified range [low, upp].\"\"\"\n",
    "    arr_min, arr_max = arr.min(), arr.max()\n",
    "    if arr_max == arr_min:\n",
    "        return np.full_like(arr, (low + upp) / 2)\n",
    "    arr_scaled = (arr - arr_min) / (arr_max - arr_min) * (upp - low) + low\n",
    "    return arr_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17aodk8wdrv",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_xgb_ranker(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    test_gold_labels,\n",
    "    group_size=6,\n",
    "    *,\n",
    "    n_estimators=500,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,     # L1\n",
    "    reg_lambda=10.0,   # L2\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate XGBRanker.\n",
    "    \n",
    "    Args:\n",
    "        group_size: Number of samples per group for ranking\n",
    "                   (e.g., 6 means every 6 consecutive samples form a ranking group)\n",
    "    \"\"\"\n",
    "    # Create group information for ranking\n",
    "    # Each group contains group_size samples to be ranked together\n",
    "    num_groups_train = len(X_train) // group_size\n",
    "    group_train = [group_size] * num_groups_train\n",
    "    \n",
    "    # Initialize XGBRanker\n",
    "    xgb_ranker = XGBRanker(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective=\"rank:pairwise\",  # Pairwise ranking objective\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Fit with group information\n",
    "    xgb_ranker.fit(\n",
    "        X_train[:num_groups_train * group_size],  # Use only complete groups\n",
    "        y_train[:num_groups_train * group_size],\n",
    "        group=group_train,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Predict - returns ranking scores\n",
    "    y_pred_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    # Scale scores back to 1-5 range\n",
    "    y_pred = scale_to_range(y_pred_scores, low=1, upp=5)\n",
    "\n",
    "    # Float eval\n",
    "    scores_float = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "\n",
    "    # Int eval\n",
    "    y_pred_int = y_pred.round().clip(1, 5).astype(int)\n",
    "    scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n",
    "\n",
    "    return scores_float, scores_int, xgb_ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "tev9sc6qmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBoost Ranker Hyperparameter Search\n",
      "================================================================================\n",
      "\n",
      "Depth=1\n",
      "--------------------------------------------------------------------------------\n",
      "  lr=0.10 n=100 group=9 | Spearman=83.2733% Acc=93.2203%\n",
      "  lr=0.05 n=200 group=9 | Spearman=82.8201% Acc=92.6554%\n",
      "  lr=0.02 n=400 group=9 | Spearman=82.7442% Acc=92.6554%\n",
      "\n",
      "Depth=2\n",
      "--------------------------------------------------------------------------------\n",
      "  lr=0.10 n=100 group=9 | Spearman=83.6578% Acc=91.5254%\n",
      "  lr=0.05 n=200 group=9 | Spearman=83.6267% Acc=92.6554%\n",
      "  lr=0.02 n=400 group=9 | Spearman=83.5746% Acc=91.5254%\n",
      "\n",
      "Depth=3\n",
      "--------------------------------------------------------------------------------\n",
      "  lr=0.10 n=100 group=9 | Spearman=83.0706% Acc=90.9605%\n",
      "  lr=0.05 n=200 group=9 | Spearman=83.1426% Acc=92.0904%\n",
      "  lr=0.02 n=400 group=9 | Spearman=83.5755% Acc=92.0904%\n"
     ]
    }
   ],
   "source": [
    "# Grid search over XGBoost Ranker hyperparameters\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBoost Ranker Hyperparameter Search\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "configs_ranker = [\n",
    "    dict(learning_rate=0.1,  n_estimators=100, group_size=9),\n",
    "    dict(learning_rate=0.05, n_estimators=200, group_size=9),\n",
    "    dict(learning_rate=0.02, n_estimators=400, group_size=9),\n",
    "]\n",
    "\n",
    "for depth in [1, 2, 3]:\n",
    "    print(f\"\\nDepth={depth}\")\n",
    "    print(\"-\" * 80)\n",
    "    for cfg in configs_ranker:\n",
    "        scores_float, scores_int, _ = train_eval_xgb_ranker(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            test_gold_labels,\n",
    "            max_depth=depth,\n",
    "            **cfg\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  lr={cfg['learning_rate']:4.2f} n={cfg['n_estimators']:3d} group={cfg['group_size']} | \"\n",
    "            f\"Spearman={scores_float['spearman']:.4%} Acc={scores_float['accuracy']:.4%}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "i85yf5pocpe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Final XGBoost Ranker Model\n",
      "================================================================================\n",
      "XGBoost Ranker (float) - Spearman: 82.7531%, Accuracy: 94.3503%\n",
      "XGBoost Ranker (int)   - Spearman: 79.8188%, Accuracy: 85.3107%\n"
     ]
    }
   ],
   "source": [
    "# Train final XGBoost Ranker with best settings\n",
    "print(\"=\" * 80)\n",
    "print(\"Final XGBoost Ranker Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    n_estimators=100,\n",
    "    max_depth=1,\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=10.0,\n",
    "    objective=\"rank:pairwise\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Create groups (samples per group)\n",
    "group_size = 9\n",
    "num_groups_train = len(X_train) // group_size\n",
    "group_train = [group_size] * num_groups_train\n",
    "\n",
    "# Train on complete groups only\n",
    "X_train_grouped = X_train[:num_groups_train * group_size]\n",
    "y_train_grouped = y_train[:num_groups_train * group_size]\n",
    "\n",
    "xgb_ranker.fit(X_train_grouped, y_train_grouped, group=group_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_ranker_scores = xgb_ranker.predict(X_test)\n",
    "y_pred_ranker = scale_to_range(y_pred_ranker_scores, low=1, upp=5)\n",
    "\n",
    "# Evaluate float version\n",
    "scores_ranker_float = evaluate_predictions_array(y_pred_ranker, test_gold_labels)\n",
    "print(f\"XGBoost Ranker (float) - Spearman: {scores_ranker_float['spearman']:.4%}, Accuracy: {scores_ranker_float['accuracy']:.4%}\")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_ranker_int = y_pred_ranker.round().clip(1, 5).astype(int)\n",
    "scores_ranker_int = evaluate_predictions_array(y_pred_ranker_int, test_gold_labels)\n",
    "print(f\"XGBoost Ranker (int)   - Spearman: {scores_ranker_int['spearman']:.4%}, Accuracy: {scores_ranker_int['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "j0poa1qx79h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Ranker Feature Importances:\n",
      "david_v1             0.1607\n",
      "david_v2             0.1645\n",
      "korean               0.2235\n",
      "urdu_v1              0.0629\n",
      "urdu_v2              0.1322\n",
      "chatgpt              0.2561\n",
      "\n",
      "Normalized Contributions (%):\n",
      "david_v1              16.07%\n",
      "david_v2              16.45%\n",
      "korean                22.35%\n",
      "urdu_v1                6.29%\n",
      "urdu_v2               13.22%\n",
      "chatgpt               25.61%\n"
     ]
    }
   ],
   "source": [
    "# Inspect XGBoost Ranker Feature Importances\n",
    "importance_ranker = xgb_ranker.feature_importances_\n",
    "print(\"XGBoost Ranker Feature Importances:\")\n",
    "for name, imp in zip(X_train.columns, importance_ranker):\n",
    "    print(f\"{name:20s} {imp:.4f}\")\n",
    "    \n",
    "# Visualize as percentages\n",
    "print(\"\\nNormalized Contributions (%):\")\n",
    "total_importance_ranker = importance_ranker.sum()\n",
    "for name, imp in zip(X_train.columns, importance_ranker):\n",
    "    print(f\"{name:20s} {imp/total_importance_ranker:>7.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecb878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SemEval25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
