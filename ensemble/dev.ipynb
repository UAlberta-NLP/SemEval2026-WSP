{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557df603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "__author__ = 'Author'\n",
    "__email__ = 'Email'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af65fa",
   "metadata": {},
   "source": [
    "# SemEval 2026 Task 5 - Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84dcf5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing...\n",
      "Starting Scoring script...\n"
     ]
    }
   ],
   "source": [
    "# dependency\n",
    "# built-in\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "\n",
    "# third-party\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from xgboost import XGBRanker, XGBRegressor\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# local - add src/eval to path for importing evaluation functions\n",
    "sys.path.insert(0, str(Path('../src/eval').resolve()))\n",
    "# Import evaluation functions from src/eval/scoring.py\n",
    "import scoring\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b742876",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c7a896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def load_predictions(filepath):\n",
    "    \"\"\"Load predictions from a JSONL file into a dictionary.\"\"\"\n",
    "    predictions = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            predictions[data['id']] = data['prediction']\n",
    "    return predictions\n",
    "\n",
    "def convert_json_to_submittable_jsonl(input_json_path, output_jsonl_path):\n",
    "    \"\"\"\n",
    "    Convert from detailed JSON format to submittable JSONL format.\n",
    "    \n",
    "    Input format (JSON array):\n",
    "        [{\"id\": \"0\", \"homonym\": \"...\", \"model_score\": 5, ...}, ...]\n",
    "    \n",
    "    Output format (JSONL):\n",
    "        {\"id\": \"0\", \"prediction\": 5}\n",
    "        {\"id\": \"1\", \"prediction\": 2}\n",
    "        ...\n",
    "    \n",
    "    Args:\n",
    "        input_json_path: Path to input JSON file (e.g., chatgpt_v1.json)\n",
    "        output_jsonl_path: Path to output JSONL file (e.g., chatgpt_v1_submission.jsonl)\n",
    "    \"\"\"\n",
    "    # Read the JSON array\n",
    "    with open(input_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Write to JSONL format\n",
    "    with open(output_jsonl_path, 'w') as f:\n",
    "        for item in data:\n",
    "            json_obj = {\n",
    "                \"id\": item[\"id\"],\n",
    "                \"prediction\": int(item[\"model_score\"])\n",
    "            }\n",
    "            f.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "    print(f\"Converted {len(data)} predictions\")\n",
    "    print(f\"Saved to: {output_jsonl_path}\")\n",
    "\n",
    "# Evaluation functions following the structure of src/eval/scoring.py\n",
    "def evaluate_predictions_array(y_pred, y_true_labels):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using the same logic as scoring.py\n",
    "    \n",
    "    Args:\n",
    "        y_pred: array of predictions\n",
    "        y_true_labels: list of gold label lists (5 ratings each)\n",
    "    \n",
    "    Returns:\n",
    "        dict with spearman and accuracy scores\n",
    "    \"\"\"\n",
    "    # Build prediction and gold lists (same structure as scoring.py)\n",
    "    pred_list = list(y_pred)\n",
    "    gold_list = [scoring.get_average(labels) for labels in y_true_labels]\n",
    "    \n",
    "    # Calculate Spearman correlation (same as scoring.py)\n",
    "    corr, p_value = spearmanr(pred_list, gold_list)\n",
    "    \n",
    "    # Calculate accuracy within SD (same logic as scoring.py)\n",
    "    correct_guesses = 0\n",
    "    wrong_guesses = 0\n",
    "    \n",
    "    for pred, labels in zip(pred_list, y_true_labels):\n",
    "        if scoring.is_within_standard_deviation(pred, labels):\n",
    "            correct_guesses += 1\n",
    "        else:\n",
    "            wrong_guesses += 1\n",
    "    \n",
    "    accuracy = correct_guesses / (correct_guesses + wrong_guesses)\n",
    "    \n",
    "    return {\n",
    "        'spearman': corr,\n",
    "        'p_value': p_value,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct_guesses,\n",
    "        'total': correct_guesses + wrong_guesses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6f7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06797395",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6014ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from pathlib import Path\n",
    "\n",
    "# base_path = '../res/results/test/*.json'\n",
    "\n",
    "# json_files = glob.glob(base_path)\n",
    "# for json_file in json_files:\n",
    "#     json_path = Path(json_file)\n",
    "#     output_path = json_file.replace('json', 'jsonl')\n",
    "#     convert_json_to_submittable_jsonl(json_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c6607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 588 gold labels\n",
      "\n",
      "Gold labels are lists of 5 human ratings (1-5 scale)\n",
      "\n",
      "Example gold labels:\n",
      "  ID 0: [4, 5, 3, 1, 5] (avg=3.60, std=1.67)\n",
      "  ID 1: [3, 3, 4, 4, 4] (avg=3.60, std=0.55)\n",
      "  ID 2: [5, 5, 2, 3, 4] (avg=3.80, std=1.30)\n",
      "  ID 3: [4, 5, 4, 3, 5] (avg=4.20, std=0.84)\n",
      "  ID 4: [1, 5, 4, 4, 1] (avg=3.00, std=1.87)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load gold labels (solution file)\n",
    "SOLUTION_FILE = Path(\"../res/data/dev_solution.jsonl\")\n",
    "\n",
    "gold_labels = {}\n",
    "with open(SOLUTION_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        gold_labels[data['id']] = data['label']\n",
    "\n",
    "print(f\"Loaded {len(gold_labels)} gold labels\")\n",
    "print(f\"\\nGold labels are lists of 5 human ratings (1-5 scale)\")\n",
    "print(f\"\\nExample gold labels:\")\n",
    "for i in range(5):\n",
    "    sample_id = str(i)\n",
    "    print(f\"  ID {sample_id}: {gold_labels[sample_id]} (avg={np.mean(gold_labels[sample_id]):.2f}, std={np.std(gold_labels[sample_id], ddof=1):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76a52dc",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb32e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 prediction files:\n",
      "  - chatgpt_v1.jsonl\n",
      "  - david_v1.jsonl\n",
      "  - david_v2.jsonl\n",
      "  - david_v3.jsonl\n",
      "  - deepseek_v1.jsonl\n",
      "  - gemini.jsonl\n",
      "  - gpt5_v1.jsonl\n",
      "  - gpt5_v3.jsonl\n",
      "  - gpt5_v4.jsonl\n",
      "  - korean_v1.jsonl\n",
      "  - korean_v2.jsonl\n",
      "  - majoritybaseline.jsonl\n",
      "  - qwen_v1.jsonl\n",
      "  - qwen_v2.jsonl\n",
      "  - randombaseline.jsonl\n",
      "  - urdu_v1.jsonl\n",
      "  - urdu_v2.jsonl\n",
      "  - urdu_v3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Path to individual system outputs\n",
    "RESULTS_DIR = Path(\"./dev/\")\n",
    "\n",
    "# Get all jsonl files\n",
    "prediction_files = sorted(RESULTS_DIR.glob(\"*.jsonl\"))\n",
    "print(f\"Found {len(prediction_files)} prediction files:\")\n",
    "for f in prediction_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e1d4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 588 predictions from chatgpt_v1\n",
      "Loaded 588 predictions from david_v1\n",
      "Loaded 588 predictions from david_v2\n",
      "Loaded 588 predictions from david_v3\n",
      "Loaded 588 predictions from deepseek_v1\n",
      "Loaded 588 predictions from gemini\n",
      "Loaded 588 predictions from gpt5_v1\n",
      "Loaded 588 predictions from gpt5_v3\n",
      "Loaded 588 predictions from gpt5_v4\n",
      "Loaded 588 predictions from korean_v1\n",
      "Loaded 588 predictions from korean_v2\n",
      "Loaded 588 predictions from majoritybaseline\n",
      "Loaded 588 predictions from qwen_v1\n",
      "Loaded 588 predictions from qwen_v2\n",
      "Loaded 588 predictions from randombaseline\n",
      "Loaded 588 predictions from urdu_v1\n",
      "Loaded 588 predictions from urdu_v2\n",
      "Loaded 588 predictions from urdu_v3\n",
      "\n",
      "Predictions DataFrame shape: (588, 18)\n",
      "Number of samples: 588\n",
      "Number of systems: 18\n",
      "\n",
      "Systems: ['chatgpt_v1', 'david_v1', 'david_v2', 'david_v3', 'deepseek_v1', 'gemini', 'gpt5_v1', 'gpt5_v3', 'gpt5_v4', 'korean_v1', 'korean_v2', 'majoritybaseline', 'qwen_v1', 'qwen_v2', 'randombaseline', 'urdu_v1', 'urdu_v2', 'urdu_v3']\n",
      "\n",
      "Sample data types:\n",
      "chatgpt_v1            int64\n",
      "david_v1              int64\n",
      "david_v2            float64\n",
      "david_v3              int64\n",
      "deepseek_v1           int64\n",
      "gemini                int64\n",
      "gpt5_v1               int64\n",
      "gpt5_v3               int64\n",
      "gpt5_v4               int64\n",
      "korean_v1           float64\n",
      "korean_v2           float64\n",
      "majoritybaseline      int64\n",
      "qwen_v1               int64\n",
      "qwen_v2               int64\n",
      "randombaseline        int64\n",
      "urdu_v1               int64\n",
      "urdu_v2               int64\n",
      "urdu_v3               int64\n",
      "dtype: object\n",
      "\n",
      "First 10 predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatgpt_v1</th>\n",
       "      <th>david_v1</th>\n",
       "      <th>david_v2</th>\n",
       "      <th>david_v3</th>\n",
       "      <th>deepseek_v1</th>\n",
       "      <th>gemini</th>\n",
       "      <th>gpt5_v1</th>\n",
       "      <th>gpt5_v3</th>\n",
       "      <th>gpt5_v4</th>\n",
       "      <th>korean_v1</th>\n",
       "      <th>korean_v2</th>\n",
       "      <th>majoritybaseline</th>\n",
       "      <th>qwen_v1</th>\n",
       "      <th>qwen_v2</th>\n",
       "      <th>randombaseline</th>\n",
       "      <th>urdu_v1</th>\n",
       "      <th>urdu_v2</th>\n",
       "      <th>urdu_v3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0600</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>4.530886</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.9400</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.586742</td>\n",
       "      <td>2.275880</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9976</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.647276</td>\n",
       "      <td>3.408030</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0024</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.392001</td>\n",
       "      <td>3.437395</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5480</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.816402</td>\n",
       "      <td>3.001934</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.4520</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.373284</td>\n",
       "      <td>3.141558</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5992</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>4.341353</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4008</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>1.744660</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.8696</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>2.118062</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.1304</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.450690</td>\n",
       "      <td>3.896887</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chatgpt_v1  david_v1  david_v2  david_v3  deepseek_v1  gemini  gpt5_v1  \\\n",
       "id                                                                           \n",
       "0            5         4    4.0600         1            4       5        5   \n",
       "1            2         4    1.9400         5            2       2        2   \n",
       "2            3         5    3.9976         1            3       4        4   \n",
       "3            2         5    2.0024         5            3       5        4   \n",
       "4            3         5    3.5480         5            3       4        3   \n",
       "5            2         4    2.4520         5            3       5        4   \n",
       "6            5         4    4.5992         1            4       5        5   \n",
       "7            1         1    1.4008         1            2       1        1   \n",
       "8            1         3    2.8696         1            4       2        1   \n",
       "9            3         3    3.1304         5            2       5        5   \n",
       "\n",
       "    gpt5_v3  gpt5_v4  korean_v1  korean_v2  majoritybaseline  qwen_v1  \\\n",
       "id                                                                      \n",
       "0         4        5   4.360252   4.530886                 4        5   \n",
       "1         1        1   2.586742   2.275880                 4        1   \n",
       "2         2        2   2.647276   3.408030                 4        3   \n",
       "3         4        4   3.392001   3.437395                 4        3   \n",
       "4         4        4   2.816402   3.001934                 4        5   \n",
       "5         4        4   3.373284   3.141558                 4        2   \n",
       "6         4        5   4.360252   4.341353                 4        1   \n",
       "7         1        1   1.942182   1.744660                 4        1   \n",
       "8         1        1   1.942182   2.118062                 4        5   \n",
       "9         4        5   3.450690   3.896887                 4        1   \n",
       "\n",
       "    qwen_v2  randombaseline  urdu_v1  urdu_v2  urdu_v3  \n",
       "id                                                      \n",
       "0         4               4        4        4        4  \n",
       "1         2               3        3        2        3  \n",
       "2         3               5        4        3        4  \n",
       "3         3               5        4        4        4  \n",
       "4         3               2        4        4        4  \n",
       "5         3               4        4        3        4  \n",
       "6         4               2        2        4        2  \n",
       "7         2               1        2        3        2  \n",
       "8         4               2        2        2        2  \n",
       "9         2               3        4        5        4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all predictions\n",
    "all_predictions = {}\n",
    "for pred_file in prediction_files:\n",
    "    system_name = pred_file.stem  # filename without extension\n",
    "    all_predictions[system_name] = load_predictions(pred_file)\n",
    "    print(f\"Loaded {len(all_predictions[system_name])} predictions from {system_name}\")\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "# Each row is a sample, each column is a system's prediction\n",
    "df_predictions = pd.DataFrame(all_predictions)\n",
    "df_predictions.index.name = 'id'\n",
    "\n",
    "print(f\"\\nPredictions DataFrame shape: {df_predictions.shape}\")\n",
    "print(f\"Number of samples: {len(df_predictions)}\")\n",
    "print(f\"Number of systems: {len(df_predictions.columns)}\")\n",
    "print(f\"\\nSystems: {list(df_predictions.columns)}\")\n",
    "print(f\"\\nSample data types:\")\n",
    "print(df_predictions.dtypes)\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "df_predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b49qoanf5i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (588, 21)\n",
      "\n",
      "Columns: ['chatgpt_v1', 'david_v1', 'david_v2', 'david_v3', 'deepseek_v1', 'gemini', 'gpt5_v1', 'gpt5_v3', 'gpt5_v4', 'korean_v1', 'korean_v2', 'majoritybaseline', 'qwen_v1', 'qwen_v2', 'randombaseline', 'urdu_v1', 'urdu_v2', 'urdu_v3', 'gold_labels', 'gold_avg', 'gold_std']\n",
      "\n",
      "First 10 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatgpt_v1</th>\n",
       "      <th>david_v1</th>\n",
       "      <th>david_v2</th>\n",
       "      <th>david_v3</th>\n",
       "      <th>deepseek_v1</th>\n",
       "      <th>gemini</th>\n",
       "      <th>gpt5_v1</th>\n",
       "      <th>gpt5_v3</th>\n",
       "      <th>gpt5_v4</th>\n",
       "      <th>korean_v1</th>\n",
       "      <th>...</th>\n",
       "      <th>majoritybaseline</th>\n",
       "      <th>qwen_v1</th>\n",
       "      <th>qwen_v2</th>\n",
       "      <th>randombaseline</th>\n",
       "      <th>urdu_v1</th>\n",
       "      <th>urdu_v2</th>\n",
       "      <th>urdu_v3</th>\n",
       "      <th>gold_labels</th>\n",
       "      <th>gold_avg</th>\n",
       "      <th>gold_std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0600</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 5, 3, 1, 5]</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.673320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.9400</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.586742</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[3, 3, 4, 4, 4]</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.547723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.9976</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.647276</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[5, 5, 2, 3, 4]</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.303840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0024</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.392001</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 5, 4, 3, 5]</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.836660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5480</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.816402</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 5, 4, 4, 1]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.870829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2.4520</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.373284</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 3, 4, 1, 3]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.224745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5992</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.360252</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 4, 5, 5, 5]</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.547723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4008</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1, 1, 2, 2, 1]</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.516398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.8696</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.942182</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 1, 1, 2, 3]</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.303840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.1304</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3.450690</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 2, 5, 4, 4]</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.095445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    chatgpt_v1  david_v1  david_v2  david_v3  deepseek_v1  gemini  gpt5_v1  \\\n",
       "id                                                                           \n",
       "0            5         4    4.0600         1            4       5        5   \n",
       "1            2         4    1.9400         5            2       2        2   \n",
       "2            3         5    3.9976         1            3       4        4   \n",
       "3            2         5    2.0024         5            3       5        4   \n",
       "4            3         5    3.5480         5            3       4        3   \n",
       "5            2         4    2.4520         5            3       5        4   \n",
       "6            5         4    4.5992         1            4       5        5   \n",
       "7            1         1    1.4008         1            2       1        1   \n",
       "8            1         3    2.8696         1            4       2        1   \n",
       "9            3         3    3.1304         5            2       5        5   \n",
       "\n",
       "    gpt5_v3  gpt5_v4  korean_v1  ...  majoritybaseline  qwen_v1  qwen_v2  \\\n",
       "id                               ...                                       \n",
       "0         4        5   4.360252  ...                 4        5        4   \n",
       "1         1        1   2.586742  ...                 4        1        2   \n",
       "2         2        2   2.647276  ...                 4        3        3   \n",
       "3         4        4   3.392001  ...                 4        3        3   \n",
       "4         4        4   2.816402  ...                 4        5        3   \n",
       "5         4        4   3.373284  ...                 4        2        3   \n",
       "6         4        5   4.360252  ...                 4        1        4   \n",
       "7         1        1   1.942182  ...                 4        1        2   \n",
       "8         1        1   1.942182  ...                 4        5        4   \n",
       "9         4        5   3.450690  ...                 4        1        2   \n",
       "\n",
       "    randombaseline  urdu_v1  urdu_v2  urdu_v3         gold_labels  gold_avg  \\\n",
       "id                                                                            \n",
       "0                4        4        4        4     [4, 5, 3, 1, 5]  3.600000   \n",
       "1                3        3        2        3     [3, 3, 4, 4, 4]  3.600000   \n",
       "2                5        4        3        4     [5, 5, 2, 3, 4]  3.800000   \n",
       "3                5        4        4        4     [4, 5, 4, 3, 5]  4.200000   \n",
       "4                2        4        4        4     [1, 5, 4, 4, 1]  3.000000   \n",
       "5                4        4        3        4     [4, 3, 4, 1, 3]  3.000000   \n",
       "6                2        2        4        2     [4, 4, 5, 5, 5]  4.600000   \n",
       "7                1        2        3        2  [1, 1, 1, 2, 2, 1]  1.333333   \n",
       "8                2        2        2        2     [4, 1, 1, 2, 3]  2.200000   \n",
       "9                3        4        5        4     [4, 2, 5, 4, 4]  3.800000   \n",
       "\n",
       "    gold_std  \n",
       "id            \n",
       "0   1.673320  \n",
       "1   0.547723  \n",
       "2   1.303840  \n",
       "3   0.836660  \n",
       "4   1.870829  \n",
       "5   1.224745  \n",
       "6   0.547723  \n",
       "7   0.516398  \n",
       "8   1.303840  \n",
       "9   1.095445  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a combined DataFrame with predictions and gold labels\n",
    "df_gold = pd.DataFrame({\n",
    "    'gold_labels': gold_labels,\n",
    "    'gold_avg': {k: np.mean(v) for k, v in gold_labels.items()},\n",
    "    'gold_std': {k: np.std(v, ddof=1) for k, v in gold_labels.items()}\n",
    "})\n",
    "\n",
    "# Combine predictions with gold labels\n",
    "df_combined = df_predictions.join(df_gold)\n",
    "\n",
    "print(f\"Combined DataFrame shape: {df_combined.shape}\")\n",
    "print(f\"\\nColumns: {list(df_combined.columns)}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "df_combined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fj1yfb0wqcp",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72891f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 588\n",
      "Training samples: 411 (69.9%)\n",
      "Test samples: 177 (30.1%)\n",
      "\n",
      "Training set gold_avg distribution:\n",
      "  Mean: 3.109\n",
      "  Std: 1.191\n",
      "  Min: 1.000\n",
      "  Max: 5.000\n",
      "\n",
      "Test set gold_avg distribution:\n",
      "  Mean: 3.140\n",
      "  Std: 1.178\n",
      "  Min: 1.000\n",
      "  Max: 5.000\n",
      "\n",
      "Training set sample IDs range: 0 to 97\n",
      "Test set sample IDs range: 1 to 99\n"
     ]
    }
   ],
   "source": [
    "# Use stratification based on binned gold_avg to ensure balanced distribution\n",
    "df_combined['gold_bin'] = pd.cut(df_combined['gold_avg'], bins=5, labels=False)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df_combined, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=df_combined['gold_bin']\n",
    ")\n",
    "\n",
    "# Drop the temporary binning column\n",
    "train_df = train_df.drop('gold_bin', axis=1)\n",
    "test_df = test_df.drop('gold_bin', axis=1)\n",
    "\n",
    "train_df.to_csv('our_dev_splits/train.tsv', sep='\\t')\n",
    "test_df.to_csv('our_dev_splits/test.tsv', sep='\\t')\n",
    "\n",
    "test_gold_labels = test_df['gold_labels'].tolist()\n",
    "\n",
    "print(f\"Total samples: {len(df_combined)}\")\n",
    "print(f\"Training samples: {len(train_df)} ({len(train_df)/len(df_combined)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(test_df)} ({len(test_df)/len(df_combined)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set gold_avg distribution:\")\n",
    "print(f\"  Mean: {train_df['gold_avg'].mean():.3f}\")\n",
    "print(f\"  Std: {train_df['gold_avg'].std():.3f}\")\n",
    "print(f\"  Min: {train_df['gold_avg'].min():.3f}\")\n",
    "print(f\"  Max: {train_df['gold_avg'].max():.3f}\")\n",
    "\n",
    "print(f\"\\nTest set gold_avg distribution:\")\n",
    "print(f\"  Mean: {test_df['gold_avg'].mean():.3f}\")\n",
    "print(f\"  Std: {test_df['gold_avg'].std():.3f}\")\n",
    "print(f\"  Min: {test_df['gold_avg'].min():.3f}\")\n",
    "print(f\"  Max: {test_df['gold_avg'].max():.3f}\")\n",
    "\n",
    "print(f\"\\nTraining set sample IDs range: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "print(f\"Test set sample IDs range: {test_df.index.min()} to {test_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dk6qijpzjlj",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_splits(train_df, test_df, model_names):\n",
    "    \"\"\"\n",
    "    Prepare training and test sets with specified models.\n",
    "\n",
    "    Args:\n",
    "        train_df: Training dataframe with all model predictions\n",
    "        test_df: Test dataframe with all model predictions\n",
    "        model_names: List of model column names to use as features\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    X_train = train_df[model_names]\n",
    "    y_train = train_df['gold_avg']\n",
    "\n",
    "    X_test = test_df[model_names]\n",
    "    y_test = test_df['gold_avg']\n",
    "\n",
    "    # print(f\"X_train shape: {X_train.shape}\")\n",
    "    # print(f\"y_train shape: {y_train.shape}\")\n",
    "    # print(f\"X_test shape: {X_test.shape}\")\n",
    "    # print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "    # print(f\"\\nSelected models: {model_names}\")\n",
    "    # print(f\"\\nFirst few training samples:\")\n",
    "    # display(pd.concat([X_train.head(), y_train.head()], axis=1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab584e1",
   "metadata": {},
   "source": [
    "# System Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4675dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Individual System Performance on Test Set (30%)\n",
      "================================================================================\n",
      "System              Spearman     Accuracy   Correct/Total\n",
      "--------------------------------------------------------------------------------\n",
      "korean_v1         0.82703251   0.90395480     160/177    \n",
      "korean_v2         0.83099820   0.89830508     159/177    \n",
      "gpt5_v1           0.77775958   0.76271186     135/177    \n",
      "gpt5_v4           0.74834361   0.73446328     130/177    \n",
      "majoritybaseline          nan   0.57627119     102/177    \n",
      "qwen_v2           0.70832985   0.80790960     143/177    \n",
      "chatgpt_v1        0.74333246   0.73446328     130/177    \n",
      "gpt5_v3           0.73301657   0.70621469     125/177    \n",
      "deepseek_v1       0.65300884   0.76271186     135/177    \n",
      "urdu_v2           0.60128077   0.70621469     125/177    \n",
      "david_v2          0.60521126   0.68361582     121/177    \n",
      "qwen_v1           0.63082859   0.64406780     114/177    \n",
      "gemini            0.63467552   0.61016949     108/177    \n",
      "urdu_v1           0.43012813   0.69491525     123/177    \n",
      "urdu_v3           0.41775234   0.69491525     123/177    \n",
      "david_v1          0.30824507   0.61016949     108/177    \n",
      "randombaseline    0.07916387   0.49717514      88/177    \n",
      "david_v3          0.20872738   0.36158192      64/177    \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_274773/1580405741.py:65: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, p_value = spearmanr(pred_list, gold_list)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each system on test set\n",
    "print(\"=\" * 80)\n",
    "print(\"Individual System Performance on Test Set (30%)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'System':<15} {'Spearman':>12} {'Accuracy':>12} {'Correct/Total':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "system_results = {}\n",
    "test_gold_labels = test_df['gold_labels'].tolist()\n",
    "\n",
    "\n",
    "all_systems = [s for s in list(df_combined.columns) if not s.startswith('gold_')]\n",
    "X_train, y_train, X_test, y_test = prepare_data_splits(train_df, test_df, all_systems)\n",
    "\n",
    "for system in all_systems:\n",
    "# for system in ['david_v1', 'david_v2', 'korean']:\n",
    "    # Get predictions from X_test\n",
    "    y_pred = X_test[system].values\n",
    "    \n",
    "    # Evaluate using official logic\n",
    "    scores = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "    system_results[system] = scores\n",
    "    \n",
    "# sort systems by average of spearman and accuracy\n",
    "sorted_systems = sorted(system_results.items(), key=lambda x: (x[1]['spearman'] + x[1]['accuracy'])/2, reverse=True)\n",
    "for system, scores in sorted_systems:\n",
    "    print(f\"{system:<15} {scores['spearman']:>12.8f} {scores['accuracy']:>12.8f} \"\n",
    "          f\"{scores['correct']:>7}/{scores['total']:<7}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qi5dte9168s",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76405638",
   "metadata": {},
   "source": [
    "## Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9699fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (float) - Spearman: 84.6561%, Accuracy: 92.0904%\n",
      "Mean (int)   - Spearman: 79.0823%, Accuracy: 82.4859%\n"
     ]
    }
   ],
   "source": [
    "# Simple Mean Ensemble: y_pred = (1/N) * sum(y_i)\n",
    "y_pred_mean = X_test.mean(axis=1).values\n",
    "\n",
    "# Evaluate float version\n",
    "scores_mean_float = evaluate_predictions_array(y_pred_mean, test_gold_labels)\n",
    "print(f\"Mean (float) - Spearman: {scores_mean_float['spearman']:.4%}, Accuracy: {scores_mean_float['accuracy']:.4%}\")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_mean_int = y_pred_mean.round().clip(1, 5).astype(int)\n",
    "scores_mean_int = evaluate_predictions_array(y_pred_mean_int, test_gold_labels)\n",
    "print(f\"Mean (int)   - Spearman: {scores_mean_int['spearman']:.4%}, Accuracy: {scores_mean_int['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4abc10",
   "metadata": {},
   "source": [
    "## Linear Stacking (Ridge Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c008e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_ridge(X_train, y_train, X_test, test_gold_labels, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Train and evaluate Ridge Regression ensemble.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_test: Test features\n",
    "        test_gold_labels: Test gold labels (list of 5 ratings each)\n",
    "        alpha: Ridge regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        scores_float, scores_int, ridge_model\n",
    "    \"\"\"\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "    # Evaluate float version\n",
    "    scores_float = evaluate_predictions_array(y_pred_ridge, test_gold_labels)\n",
    "\n",
    "    # Evaluate integer version\n",
    "    y_pred_int = y_pred_ridge.round().clip(1, 5).astype(int)\n",
    "    scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n",
    "\n",
    "    return scores_float, scores_int, ridge\n",
    "\n",
    "\n",
    "def grid_search_ridge(X_train, y_train, X_test, test_gold_labels, alphas=[0.1, 1.0, 10.0], verbose=True):\n",
    "    \"\"\"\n",
    "    Grid search over Ridge alpha values.\n",
    "\n",
    "    Args:\n",
    "        X_train, y_train, X_test, test_gold_labels: Data splits\n",
    "        alphas: List of alpha values to try\n",
    "        verbose: Whether to print detailed progress\n",
    "\n",
    "    Returns:\n",
    "        best_alpha, best_scores_float, best_scores_int, best_model\n",
    "    \"\"\"\n",
    "    best_avg_score = 0\n",
    "    best_alpha = None\n",
    "    best_scores_float = None\n",
    "    best_scores_int = None\n",
    "    best_model = None\n",
    "\n",
    "    for alpha in alphas:\n",
    "        scores_float, scores_int, model = train_eval_ridge(\n",
    "            X_train, y_train, X_test, test_gold_labels, alpha=alpha\n",
    "        )\n",
    "\n",
    "        avg_score = (scores_float['spearman'] + scores_float['accuracy']) / 2\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   α={alpha:5.1f} → Spearman: {scores_float['spearman']:.2%}, \"\n",
    "                  f\"Accuracy: {scores_float['accuracy']:.2%}, \"\n",
    "                  f\"Avg: {avg_score:.2%}\")\n",
    "\n",
    "        if avg_score > best_avg_score:\n",
    "            best_avg_score = avg_score\n",
    "            best_alpha = alpha\n",
    "            best_scores_float = scores_float\n",
    "            best_scores_int = scores_int\n",
    "            best_model = model\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n   ✓ Best: α={best_alpha} with average score {best_avg_score:.2%}\")\n",
    "\n",
    "    return best_alpha, best_scores_float, best_scores_int, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1gmporf5msw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_combination(combination_name, model_names, train_df, test_df, test_gold_labels, alphas=[0.1, 1.0, 10.0]):\n",
    "    \"\"\"\n",
    "    Test a specific model combination with Ridge regression.\n",
    "\n",
    "    Args:\n",
    "        combination_name: Name of the combination (for display)\n",
    "        model_names: List of model column names to use\n",
    "        train_df, test_df: Data splits\n",
    "        test_gold_labels: Gold labels for test set\n",
    "        alphas: Alpha values to try\n",
    "\n",
    "    Returns:\n",
    "        best_alpha, best_scores_float, best_scores_int, best_model, X_train, X_test\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X_train = train_df[model_names]\n",
    "    y_train = train_df['gold_avg']\n",
    "    X_test = test_df[model_names]\n",
    "    y_test = test_df['gold_avg']\n",
    "\n",
    "    # Run grid search\n",
    "    best_alpha, best_scores_float, best_scores_int, best_model = grid_search_ridge(\n",
    "        X_train, y_train, X_test, test_gold_labels, alphas=alphas, verbose=False\n",
    "    )\n",
    "\n",
    "    return best_alpha, best_scores_float, best_scores_int, best_model, X_train, X_test\n",
    "\n",
    "def compare_all_combinations(train_df, test_df, test_gold_labels, model_combinations, alphas=[0.1, 1.0, 10.0]):\n",
    "    \"\"\"\n",
    "    Compare all model combinations and summarize results.\n",
    "\n",
    "    Args:\n",
    "        train_df, test_df: Data splits\n",
    "        test_gold_labels: Gold labels for test set\n",
    "        model_combinations: Dict of {name: [model_list]}\n",
    "        alphas: Alpha values to try\n",
    "\n",
    "    Returns:\n",
    "        results_summary: DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for combo_name, model_names in model_combinations.items():\n",
    "        best_alpha, scores_float, scores_int, model, X_train, X_test = test_model_combination(\n",
    "            combo_name, model_names, train_df, test_df, test_gold_labels, alphas\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'combination': combo_name,\n",
    "            'num_models': len(model_names),\n",
    "            'models': ', '.join(model_names),\n",
    "            'best_alpha': best_alpha,\n",
    "            'spearman': scores_float['spearman'],\n",
    "            'accuracy': scores_float['accuracy'],\n",
    "            'avg_score': (scores_float['spearman'] + scores_float['accuracy']) / 2,\n",
    "        })\n",
    "\n",
    "    # Create summary DataFrame sorted by average score\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    summary_df = summary_df.sort_values('avg_score', ascending=False)\n",
    "    \n",
    "    # Display table ranked by average\n",
    "    print(\"\\n📋 Results (Ranked by Average Score):\")\n",
    "    display_df = summary_df[['combination', 'num_models', 'avg_score', 'spearman', 'accuracy', 'best_alpha']].copy()\n",
    "    display_df.columns = ['Combination', '#Models', 'Avg Score', 'Spearman', 'Accuracy', 'Best α']\n",
    "    \n",
    "    # Format percentages\n",
    "    for col in ['Avg Score', 'Spearman', 'Accuracy']:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.2%}\")\n",
    "    \n",
    "    display(display_df.reset_index(drop=True))\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t558ydsmgla",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Results (Ranked by Average Score):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Combination</th>\n",
       "      <th>#Models</th>\n",
       "      <th>Avg Score</th>\n",
       "      <th>Spearman</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Best α</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chatgpt_v1+david_v2+deepseek_v1+gemini+gpt5_v1...</td>\n",
       "      <td>10</td>\n",
       "      <td>89.08%</td>\n",
       "      <td>84.37%</td>\n",
       "      <td>93.79%</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Combination  #Models Avg Score  \\\n",
       "0  chatgpt_v1+david_v2+deepseek_v1+gemini+gpt5_v1...       10    89.08%   \n",
       "\n",
       "  Spearman Accuracy  Best α  \n",
       "0   84.37%   93.79%       5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combinations_to_compare = {}\n",
    "\n",
    "# for size in range(2, len(all_systems) + 1):  # Start from 2 to skip single models\n",
    "for combo in [['chatgpt_v1','david_v2','deepseek_v1','gemini','gpt5_v1','gpt5_v3','gpt5_v4','korean_v2','qwen_v1','urdu_v2']]:# combinations(all_systems,):\n",
    "        # Use shortened names for cleaner output\n",
    "        combo_name = '+'.join(combo)\n",
    "        combinations_to_compare[combo_name] = list(combo)\n",
    "\n",
    "summary = compare_all_combinations(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    test_gold_labels,\n",
    "    combinations_to_compare,\n",
    "    alphas=[10.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ceeb779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Ensemble evaluation complete.\n",
      "\n",
      "Summary of all model combinations evaluated:\n",
      "                               combination  num_models                                         models  best_alpha  spearman  accuracy  avg_score\n",
      "david_v2+gpt5_v1+korean_v2+qwen_v1+urdu_v2           5 david_v2, gpt5_v1, korean_v2, qwen_v1, urdu_v2        0.01  0.846767  0.932203   0.889485\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Ensemble evaluation complete.\")\n",
    "print(\"\\nSummary of all model combinations evaluated:\")\n",
    "# row 1\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "vx2c5kcubyk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Compare all predefined combinations\n",
    "# Uncomment and run to compare all combinations:\n",
    "\n",
    "# summary = compare_all_combinations(\n",
    "#     train_df,\n",
    "#     test_df,\n",
    "#     test_gold_labels,\n",
    "#     MODEL_COMBINATIONS\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "pwox2h1rpc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Test a single combination\n",
    "# Uncomment and run to test just one combination:\n",
    "\n",
    "# test_model_combination(\n",
    "#     'best_5',\n",
    "#     MODEL_COMBINATIONS['best_5'],\n",
    "#     train_df,\n",
    "#     test_df,\n",
    "#     test_gold_labels\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae292105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect Ridge Weights\n",
    "# coef = ridge.coef_\n",
    "# for name, w in zip(X_train.columns, coef):\n",
    "#     print(f\"{name:20s} {w:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "324b574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# david_v1+david_v2+korean_v1+korean_v2+qwen_v1+qwen_v2+urdu_v2\n",
    "best_models = [ 'david_v1', 'david_v2', 'korean_v1', 'korean_v2', 'qwen_v1', 'qwen_v2', 'urdu_v2' ]\n",
    "X_train, y_train, X_test, y_test = prepare_data_splits(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    best_models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156pqmx8rme",
   "metadata": {},
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f9b1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_xgb(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    test_gold_labels,\n",
    "    *,\n",
    "    n_estimators=500,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,     # L1\n",
    "    reg_lambda=10.0,   # L2\n",
    "    random_state=42,\n",
    "):\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Fit with early stopping (VERY IMPORTANT)\n",
    "    xgb.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    y_pred = xgb.predict(X_test)\n",
    "\n",
    "    # Float eval\n",
    "    scores_float = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "\n",
    "    # Int eval\n",
    "    y_pred_int = y_pred.round().clip(1, 5).astype(int)\n",
    "    scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n",
    "\n",
    "    return scores_float, scores_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "07e178af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_as_needed(list_o_thing, factor):\n",
    "    \n",
    "\n",
    "    is_array = isinstance(list_o_thing, np.ndarray)\n",
    "\n",
    "    \n",
    "    ans = [] if not is_array else np.empty_like(list_o_thing, dtype=float)\n",
    "\n",
    "    for i, guy in enumerate(list_o_thing):\n",
    "        if guy < 2:\n",
    "            new_guy = 2 - (2 - guy) * factor\n",
    "            assert new_guy <= 2 and new_guy >= guy\n",
    "        elif guy > 4:\n",
    "            new_guy = 4 + (guy - 4) * factor\n",
    "            assert new_guy >= 4 and new_guy <= guy\n",
    "        else:\n",
    "            new_guy = guy  # untouched inside [2, 4]\n",
    "\n",
    "        if is_array:\n",
    "            ans[i] = new_guy\n",
    "        else:\n",
    "            ans.append(float(new_guy))\n",
    "   \n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "12b7ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_with_less_scaling(a, b):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    assert False\n",
    "\n",
    "def any_self_with_less_scaling_is_worse(r, p):\n",
    "    \n",
    "    for guy in r:\n",
    "        if guy != p and is_same_with_less_scaling(guy, p):\n",
    "            assert guy['avg_score'] <= p['avg_score']\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64857a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def grid_search_xgboost(test_gold_labels, param_grid=None):\n",
    "    \"\"\"\n",
    "    Grid search over XGBoost hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_test, test_gold_labels: Test data\n",
    "        param_grid: Dictionary of parameters to search. If None, uses default grid.\n",
    "    \n",
    "    Returns:\n",
    "        best_params, best_scores_float, best_scores_int, best_model, results_df\n",
    "    \"\"\"\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    # Default parameter grid\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 500],\n",
    "            'max_depth': [1, 2, 3],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'min_child_weight': [10, 50, 100],\n",
    "            'subsample': [0.7, 0.8, 0.9],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "            'reg_alpha': [0.1, 1.0, 10.0],\n",
    "            'reg_lambda': [1.0, 10.0, 50.0],\n",
    "        }\n",
    "    \n",
    "    results = []\n",
    "    best_avg_score = 0\n",
    "    best_params = None\n",
    "    best_scores_float = None\n",
    "    best_scores_int = None\n",
    "    best_model = None\n",
    "    \n",
    "    # Generate all combinations\n",
    "    import itertools\n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    \n",
    "    total_combinations = 1\n",
    "    for v in values:\n",
    "        total_combinations *= len(v)\n",
    "    \n",
    "    \n",
    "    total = math.prod(len(v) for v in values)\n",
    "\n",
    "    \n",
    "    for i, combination in enumerate(\n",
    "    tqdm(product(*values), total=total),\n",
    "    start=1\n",
    "    ):\n",
    "        params = dict(zip(keys, combination))\n",
    "        \n",
    "        model_names = params['model_names']\n",
    "        \n",
    "        X_train_curr, y_train_curr, X_test_curr, y_test_curr = prepare_data_splits(train_df, test_df, model_names)\n",
    "        \n",
    "        # Train XGBoost with these parameters\n",
    "        xgb = XGBRegressor(\n",
    "            n_estimators=params['n_estimators'],\n",
    "            max_depth=params['max_depth'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            min_child_weight=params['min_child_weight'],\n",
    "            subsample=params['subsample'],\n",
    "            colsample_bytree=params['colsample_bytree'],\n",
    "            reg_alpha=params['reg_alpha'],\n",
    "            reg_lambda=params['reg_lambda'],\n",
    "            objective=\"reg:squarederror\",\n",
    "            random_state=0,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        \n",
    "        xgb.fit(X_train_curr, y_train_curr, verbose=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = compress_as_needed(xgb.predict(X_test_curr), params['rescale'])\n",
    "        scores_float = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "        \n",
    "        y_pred_int = y_pred.round().clip(1, 5).astype(int)\n",
    "        scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n",
    "        \n",
    "        avg_score = (scores_float['spearman'] + scores_float['accuracy']) / 2\n",
    "        \n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            **params,\n",
    "            'spearman': scores_float['spearman'],\n",
    "            'accuracy': scores_float['accuracy'],\n",
    "            'avg_score': avg_score,\n",
    "        })\n",
    "        \n",
    "        # Track best\n",
    "        if avg_score > best_avg_score:\n",
    "            best_avg_score = avg_score\n",
    "            best_params = params\n",
    "            best_scores_float = scores_float\n",
    "            best_scores_int = scores_int\n",
    "            best_model = xgb\n",
    "        \n",
    "        # Progress update every 10%\n",
    "        if i % max(1, total_combinations // 1000) == 0:\n",
    "            # print(f\"  Progress: {i}/{total_combinations} ({i/total_combinations*100:.0f}%)\")\n",
    "            pass\n",
    "        \n",
    "        for curr_param in results:\n",
    "            any_self_with_less_scaling_is_worse(results, curr_param)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    \n",
    "    results_df = results_df.sort_values('avg_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n✅ Best parameters found:\")\n",
    "    for key, val in best_params.items():\n",
    "        print(f\"   {key:20s} = {val}\")\n",
    "    print(f\"\\n   Avg Score: {best_avg_score:.2%}\")\n",
    "    print(f\"   Spearman:  {best_scores_float['spearman']:.2%}\")\n",
    "    print(f\"   Accuracy:  {best_scores_float['accuracy']:.2%}\")\n",
    "    \n",
    "    return best_params, best_scores_float, best_scores_int, best_model, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf20a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KOREAN_PARTS = [\n",
    "    'korean_ambiguity',\n",
    "    'korean_correct',\n",
    "    'korean_neither',\n",
    "    'korean_preference',\n",
    "    'korean_rating',\n",
    "]\n",
    "\n",
    "def only_combined_korean(s):\n",
    "    return (not any(p in s for p in KOREAN_PARTS)) and ('korean_v' in s)\n",
    "\n",
    "def all_components_korean(s):\n",
    "    return (all(p in s for p in KOREAN_PARTS)) and ('korean_v' not in s)\n",
    "\n",
    "\n",
    "def this_is_a_good_combo(c):\n",
    "    \n",
    "    s = str(c)\n",
    " \n",
    "    for required in ['david', 'korean', 'urdu', 'v2']:\n",
    "        if required not in str(c):\n",
    "            return False\n",
    "    if 'qw' not in str(c) and 'gpt5' not in str(c) and 'deep' not in str(c):\n",
    "        return False\n",
    "    if 'david_v1' in str(c) or 'david_v3' in str(c):\n",
    "        return False\n",
    "    if 'urdu_v1' in str(c) or  'urdu_v3' in str(c):\n",
    "        return False\n",
    "    if 'korean_v2' in str(c):\n",
    "        return False      \n",
    "    if only_combined_korean(s) == all_components_korean(s):\n",
    "        return False\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "96dcc60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/11520 [00:00<20:16,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'max_depth': 1, 'rescale': 0, 'learning_rate': 0.01, 'min_child_weight': 30, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'model_names': ['david_v2', 'deepseek_v1', 'korean_v1', 'urdu_v2'], 'spearman': np.float64(0.8295323786993674), 'accuracy': 0.8305084745762712, 'avg_score': np.float64(0.8300204266378193)}\n",
      "{'n_estimators': 500, 'max_depth': 1, 'rescale': 0, 'learning_rate': 0.01, 'min_child_weight': 30, 'subsample': 0.7, 'colsample_bytree': 0.7, 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'model_names': ['david_v2', 'gpt5_v1', 'korean_v1', 'urdu_v2']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     13\u001b[39m         fff.write(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(comb_to_comp) + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m custom_grid = {\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m500\u001b[39m],\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_names\u001b[39m\u001b[33m'\u001b[39m: combinations_to_compare\n\u001b[32m     28\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m best_params, scores_float, scores_int, best_model, results_df = \u001b[43mgrid_search_xgboost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_gold_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_grid\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# View top 10 parameter combinations\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📋 Top 10 Parameter Combinations:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mgrid_search_xgboost\u001b[39m\u001b[34m(test_gold_labels, param_grid)\u001b[39m\n\u001b[32m     86\u001b[39m scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n\u001b[32m     88\u001b[39m avg_score = (scores_float[\u001b[33m'\u001b[39m\u001b[33mspearman\u001b[39m\u001b[33m'\u001b[39m] + scores_float[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]) / \u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43many_self_with_less_scaling_is_worse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     92\u001b[39m results.append({\n\u001b[32m     93\u001b[39m     **params,\n\u001b[32m     94\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mspearman\u001b[39m\u001b[33m'\u001b[39m: scores_float[\u001b[33m'\u001b[39m\u001b[33mspearman\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     95\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: scores_float[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     96\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mavg_score\u001b[39m\u001b[33m'\u001b[39m: avg_score,\n\u001b[32m     97\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36many_self_with_less_scaling_is_worse\u001b[39m\u001b[34m(r, p)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34many_self_with_less_scaling_is_worse\u001b[39m(r, p):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m guy \u001b[38;5;129;01min\u001b[39;00m r:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m guy != p \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mis_same_with_less_scaling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     10\u001b[39m             \u001b[38;5;28;01massert\u001b[39;00m guy[\u001b[33m'\u001b[39m\u001b[33mavg_score\u001b[39m\u001b[33m'\u001b[39m] <= p[\u001b[33m'\u001b[39m\u001b[33mavg_score\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mis_same_with_less_scaling\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "combinations_to_compare = []\n",
    "for size in range(2, len(all_systems) + 2):  # Start from 2 to skip single models\n",
    "    for combo in combinations(all_systems, size):\n",
    "        # Use shortened names for cleaner output\n",
    "        if this_is_a_good_combo(combo):\n",
    "            combo_name = '+'.join(combo)\n",
    "            combinations_to_compare.append(list(combo))\n",
    "\n",
    "\n",
    "\n",
    "with open('combstocomp.out', 'w', encoding='utf-8') as fff:\n",
    "    for comb_to_comp in combinations_to_compare:\n",
    "        fff.write(' '.join(comb_to_comp) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "custom_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [1],\n",
    "    'rescale': [0, 0.01, 0.25, 1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_child_weight': [30, 50],\n",
    "    'subsample': [0.7, 0.8],\n",
    "    'colsample_bytree': [0.7, 0.8],\n",
    "    'reg_alpha': [1.0, 10.0],\n",
    "    'reg_lambda': [1.0, 10.0],\n",
    "    'model_names': combinations_to_compare\n",
    "}\n",
    "\n",
    "best_params, scores_float, scores_int, best_model, results_df = grid_search_xgboost(\n",
    "    test_gold_labels, custom_grid\n",
    ")\n",
    "\n",
    "# View top 10 parameter combinations\n",
    "print(\"\\n📋 Top 10 Parameter Combinations:\")\n",
    "display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5e61524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = 0\n",
    "with open(\"KEEPTHISINMINDOK5.out\", 'w', encoding='utf-8') as in_f:\n",
    "    for index, row in results_df.iterrows():\n",
    "        if a < 10000:\n",
    "            in_f.write(str(row['model_names']) + '\\n')\n",
    "            in_f.write(str(row) + '\\n\\n\\n\\n')\n",
    "        a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9350302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost depth=1 (float) - Average: 88.8299%, Spearman: 83.3095%, Accuracy: 94.3503%\n",
      "XGBoost depth=1 (int)   - Average: 80.2304%, Spearman: 77.4100%, Accuracy: 83.0508%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost with competition-validated settings\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,          # matched to lr=0.1\n",
    "    max_depth=1,               # CRITICAL: best Spearman\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=30,       # strong regularization\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=10.0,             # L1 regularization\n",
    "    reg_lambda=10.0,           # L2 regularization\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Train on training set\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate float version\n",
    "scores_xgb_float = evaluate_predictions_array(y_pred_xgb, test_gold_labels)\n",
    "print(\n",
    "    f\"XGBoost depth=1 (float) - \"\n",
    "    f\"Average: {(scores_xgb_float['spearman'] + scores_xgb_float['accuracy'])/2:.4%}, \"\n",
    "    f\"Spearman: {scores_xgb_float['spearman']:.4%}, \"\n",
    "    f\"Accuracy: {scores_xgb_float['accuracy']:.4%}\"\n",
    ")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_xgb_int = y_pred_xgb.round().clip(1, 5).astype(int)\n",
    "scores_xgb_int = evaluate_predictions_array(y_pred_xgb_int, test_gold_labels)\n",
    "print(\n",
    "    f\"XGBoost depth=1 (int)   - \"\n",
    "    f\"Average: {(scores_xgb_int['spearman'] + scores_xgb_int['accuracy'])/2:.4%}, \"\n",
    "    f\"Spearman: {scores_xgb_int['spearman']:.4%}, \"\n",
    "    f\"Accuracy: {scores_xgb_int['accuracy']:.4%}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "zvuvrblfn3k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Feature Importances:\n",
      "david_v2             0.0884\n",
      "korean_v1            0.1413\n",
      "korean_v2            0.7069\n",
      "urdu_v2              0.0493\n",
      "qwen_v2              0.0140\n",
      "\n",
      "Normalized Contributions (%):\n",
      "david_v2               8.84%\n",
      "korean_v1             14.13%\n",
      "korean_v2             70.69%\n",
      "urdu_v2                4.93%\n",
      "qwen_v2                1.40%\n"
     ]
    }
   ],
   "source": [
    "# Inspect XGBoost Feature Importances\n",
    "importance = xgb.feature_importances_\n",
    "print(\"XGBoost Feature Importances:\")\n",
    "for name, imp in zip(X_train.columns, importance):\n",
    "    print(f\"{name:20s} {imp:.4f}\")\n",
    "    \n",
    "# Visualize as percentages\n",
    "print(\"\\nNormalized Contributions (%):\")\n",
    "total_importance = importance.sum()\n",
    "for name, imp in zip(X_train.columns, importance):\n",
    "    print(f\"{name:20s} {imp/total_importance:>7.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k0dkxcpsqn",
   "metadata": {},
   "source": [
    "## XGBoost Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "wdtsm4hle1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to scale predictions back to 1-5 range\n",
    "def scale_to_range(arr, low=1, upp=5):\n",
    "    \"\"\"Scale array values to be within the specified range [low, upp].\"\"\"\n",
    "    arr_min, arr_max = arr.min(), arr.max()\n",
    "    if arr_max == arr_min:\n",
    "        return np.full_like(arr, (low + upp) / 2)\n",
    "    arr_scaled = (arr - arr_min) / (arr_max - arr_min) * (upp - low) + low\n",
    "    return arr_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "17aodk8wdrv",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_xgb_ranker(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    test_gold_labels,\n",
    "    group_size=6,\n",
    "    *,\n",
    "    n_estimators=500,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,     # L1\n",
    "    reg_lambda=10.0,   # L2\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate XGBRanker.\n",
    "    \n",
    "    Args:\n",
    "        group_size: Number of samples per group for ranking\n",
    "                   (e.g., 6 means every 6 consecutive samples form a ranking group)\n",
    "    \"\"\"\n",
    "    # Create group information for ranking\n",
    "    # Each group contains group_size samples to be ranked together\n",
    "    num_groups_train = len(X_train) // group_size\n",
    "    group_train = [group_size] * num_groups_train\n",
    "    \n",
    "    # Initialize XGBRanker\n",
    "    xgb_ranker = XGBRanker(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        min_child_weight=min_child_weight,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        objective=\"rank:pairwise\",  # Pairwise ranking objective\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Fit with group information\n",
    "    xgb_ranker.fit(\n",
    "        X_train[:num_groups_train * group_size],  # Use only complete groups\n",
    "        y_train[:num_groups_train * group_size],\n",
    "        group=group_train,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Predict - returns ranking scores\n",
    "    y_pred_scores = xgb_ranker.predict(X_test)\n",
    "    \n",
    "    # Scale scores back to 1-5 range\n",
    "    y_pred = scale_to_range(y_pred_scores, low=1, upp=5)\n",
    "\n",
    "    # Float eval\n",
    "    scores_float = evaluate_predictions_array(y_pred, test_gold_labels)\n",
    "\n",
    "    # Int eval\n",
    "    y_pred_int = y_pred.round().clip(1, 5).astype(int)\n",
    "    scores_int = evaluate_predictions_array(y_pred_int, test_gold_labels)\n",
    "\n",
    "    return scores_float, scores_int, xgb_ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "tev9sc6qmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "XGBoost Ranker Hyperparameter Search\n",
      "================================================================================\n",
      "\n",
      "Depth=1\n",
      "--------------------------------------------------------------------------------\n",
      "  lr=0.10 n=100 group=6 | Spearman=83.3431% Acc=92.0904%\n",
      "  lr=0.05 n=200 group=6 | Spearman=83.5367% Acc=93.7853%\n",
      "  lr=0.02 n=400 group=6 | Spearman=83.7295% Acc=92.6554%\n",
      "\n",
      "Depth=2\n",
      "--------------------------------------------------------------------------------\n",
      "  lr=0.10 n=100 group=6 | Spearman=82.9971% Acc=88.7006%\n",
      "  lr=0.05 n=200 group=6 | Spearman=82.9917% Acc=90.9605%\n",
      "  lr=0.02 n=400 group=6 | Spearman=83.0810% Acc=89.8305%\n",
      "\n",
      "Depth=3\n",
      "--------------------------------------------------------------------------------\n",
      "  lr=0.10 n=100 group=6 | Spearman=82.6533% Acc=90.3955%\n",
      "  lr=0.05 n=200 group=6 | Spearman=83.0643% Acc=89.2655%\n",
      "  lr=0.02 n=400 group=6 | Spearman=82.8186% Acc=89.2655%\n"
     ]
    }
   ],
   "source": [
    "# Grid search over XGBoost Ranker hyperparameters\n",
    "print(\"=\" * 80)\n",
    "print(\"XGBoost Ranker Hyperparameter Search\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "configs_ranker = [\n",
    "    dict(learning_rate=0.1,  n_estimators=100, group_size=6),\n",
    "    dict(learning_rate=0.05, n_estimators=200, group_size=6),\n",
    "    dict(learning_rate=0.02, n_estimators=400, group_size=6),\n",
    "]\n",
    "\n",
    "for depth in [1, 2, 3]:\n",
    "    print(f\"\\nDepth={depth}\")\n",
    "    print(\"-\" * 80)\n",
    "    for cfg in configs_ranker:\n",
    "        scores_float, scores_int, _ = train_eval_xgb_ranker(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            test_gold_labels,\n",
    "            max_depth=depth,\n",
    "            **cfg\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  lr={cfg['learning_rate']:4.2f} n={cfg['n_estimators']:3d} group={cfg['group_size']} | \"\n",
    "            f\"Spearman={scores_float['spearman']:.4%} Acc={scores_float['accuracy']:.4%}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "i85yf5pocpe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Final XGBoost Ranker Model\n",
      "================================================================================\n",
      "XGBoost Ranker (float) - Average: 88.6203%, Spearman: 83.4553%, Accuracy: 93.7853%\n",
      "XGBoost Ranker (int)   - Average: 83.7981%, Spearman: 81.1555%, Accuracy: 86.4407%\n"
     ]
    }
   ],
   "source": [
    "# Train final XGBoost Ranker with best settings\n",
    "print(\"=\" * 80)\n",
    "print(\"Final XGBoost Ranker Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "xgb_ranker = XGBRanker(\n",
    "    n_estimators=400,\n",
    "    max_depth=1,\n",
    "    learning_rate=0.05,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=10.0,\n",
    "    objective=\"rank:pairwise\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Create groups (samples per group)\n",
    "group_size = 6\n",
    "num_groups_train = len(X_train) // group_size\n",
    "group_train = [group_size] * num_groups_train\n",
    "\n",
    "# Train on complete groups only\n",
    "X_train_grouped = X_train[:num_groups_train * group_size]\n",
    "y_train_grouped = y_train[:num_groups_train * group_size]\n",
    "\n",
    "xgb_ranker.fit(X_train_grouped, y_train_grouped, group=group_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_ranker_scores = xgb_ranker.predict(X_test)\n",
    "y_pred_ranker = scale_to_range(y_pred_ranker_scores, low=1, upp=5)\n",
    "\n",
    "# Evaluate float version\n",
    "scores_ranker_float = evaluate_predictions_array(y_pred_ranker, test_gold_labels)\n",
    "print(f\"XGBoost Ranker (float) - Average: {(scores_ranker_float['spearman'] + scores_ranker_float['accuracy'])/2:.4%}, \"\n",
    "      f\"Spearman: {scores_ranker_float['spearman']:.4%}, Accuracy: {scores_ranker_float['accuracy']:.4%}\")\n",
    "\n",
    "# Evaluate integer version\n",
    "y_pred_ranker_int = y_pred_ranker.round().clip(1, 5).astype(int)\n",
    "scores_ranker_int = evaluate_predictions_array(y_pred_ranker_int, test_gold_labels)\n",
    "print(f\"XGBoost Ranker (int)   - Average: {(scores_ranker_int['spearman'] + scores_ranker_int['accuracy'])/2:.4%}, \"\n",
    "      f\"Spearman: {scores_ranker_int['spearman']:.4%}, Accuracy: {scores_ranker_int['accuracy']:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "j0poa1qx79h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Ranker Feature Importances:\n",
      "david_v2             0.1766\n",
      "korean_v1            0.2209\n",
      "korean_v2            0.2235\n",
      "urdu_v2              0.1518\n",
      "qwen_v2              0.2271\n",
      "\n",
      "Normalized Contributions (%):\n",
      "david_v2              17.66%\n",
      "korean_v1             22.09%\n",
      "korean_v2             22.35%\n",
      "urdu_v2               15.18%\n",
      "qwen_v2               22.71%\n"
     ]
    }
   ],
   "source": [
    "# Inspect XGBoost Ranker Feature Importances\n",
    "importance_ranker = xgb_ranker.feature_importances_\n",
    "print(\"XGBoost Ranker Feature Importances:\")\n",
    "for name, imp in zip(X_train.columns, importance_ranker):\n",
    "    print(f\"{name:20s} {imp:.4f}\")\n",
    "    \n",
    "# Visualize as percentages\n",
    "print(\"\\nNormalized Contributions (%):\")\n",
    "total_importance_ranker = importance_ranker.sum()\n",
    "for name, imp in zip(X_train.columns, importance_ranker):\n",
    "    print(f\"{name:20s} {imp/total_importance_ranker:>7.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecb878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
